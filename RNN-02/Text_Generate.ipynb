{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  147748\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  147648\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.9757Epoch 00001: loss improved from inf to 2.97561, saving model to weights-improvement-01-2.9756.hdf5\n",
      "147648/147648 [==============================] - 528s 4ms/step - loss: 2.9756\n",
      "Epoch 2/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.7456Epoch 00002: loss improved from 2.97561 to 2.74554, saving model to weights-improvement-02-2.7455.hdf5\n",
      "147648/147648 [==============================] - 547s 4ms/step - loss: 2.7455\n",
      "Epoch 3/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.6339Epoch 00003: loss improved from 2.74554 to 2.63389, saving model to weights-improvement-03-2.6339.hdf5\n",
      "147648/147648 [==============================] - 553s 4ms/step - loss: 2.6339\n",
      "Epoch 4/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.5595Epoch 00004: loss improved from 2.63389 to 2.55929, saving model to weights-improvement-04-2.5593.hdf5\n",
      "147648/147648 [==============================] - 553s 4ms/step - loss: 2.5593\n",
      "Epoch 5/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.4848Epoch 00005: loss improved from 2.55929 to 2.48473, saving model to weights-improvement-05-2.4847.hdf5\n",
      "147648/147648 [==============================] - 569s 4ms/step - loss: 2.4847\n",
      "Epoch 6/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.4322Epoch 00006: loss improved from 2.48473 to 2.43212, saving model to weights-improvement-06-2.4321.hdf5\n",
      "147648/147648 [==============================] - 567s 4ms/step - loss: 2.4321\n",
      "Epoch 7/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.3740Epoch 00007: loss improved from 2.43212 to 2.37414, saving model to weights-improvement-07-2.3741.hdf5\n",
      "147648/147648 [==============================] - 650s 4ms/step - loss: 2.3741\n",
      "Epoch 8/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.3267Epoch 00008: loss improved from 2.37414 to 2.32674, saving model to weights-improvement-08-2.3267.hdf5\n",
      "147648/147648 [==============================] - 671s 5ms/step - loss: 2.3267\n",
      "Epoch 9/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.2803Epoch 00009: loss improved from 2.32674 to 2.28025, saving model to weights-improvement-09-2.2803.hdf5\n",
      "147648/147648 [==============================] - 625s 4ms/step - loss: 2.2803\n",
      "Epoch 10/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.2348Epoch 00010: loss improved from 2.28025 to 2.23485, saving model to weights-improvement-10-2.2348.hdf5\n",
      "147648/147648 [==============================] - 581s 4ms/step - loss: 2.2348\n",
      "Epoch 11/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.2055Epoch 00011: loss improved from 2.23485 to 2.20557, saving model to weights-improvement-11-2.2056.hdf5\n",
      "147648/147648 [==============================] - 572s 4ms/step - loss: 2.2056\n",
      "Epoch 12/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.1581Epoch 00012: loss improved from 2.20557 to 2.15797, saving model to weights-improvement-12-2.1580.hdf5\n",
      "147648/147648 [==============================] - 561s 4ms/step - loss: 2.1580\n",
      "Epoch 13/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.1217Epoch 00013: loss improved from 2.15797 to 2.12184, saving model to weights-improvement-13-2.1218.hdf5\n",
      "147648/147648 [==============================] - 568s 4ms/step - loss: 2.1218\n",
      "Epoch 14/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.0859Epoch 00014: loss improved from 2.12184 to 2.08592, saving model to weights-improvement-14-2.0859.hdf5\n",
      "147648/147648 [==============================] - 563s 4ms/step - loss: 2.0859\n",
      "Epoch 15/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.0485Epoch 00015: loss improved from 2.08592 to 2.04854, saving model to weights-improvement-15-2.0485.hdf5\n",
      "147648/147648 [==============================] - 561s 4ms/step - loss: 2.0485\n",
      "Epoch 16/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 2.0177Epoch 00016: loss improved from 2.04854 to 2.01779, saving model to weights-improvement-16-2.0178.hdf5\n",
      "147648/147648 [==============================] - 640s 4ms/step - loss: 2.0178\n",
      "Epoch 17/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 1.9860Epoch 00017: loss improved from 2.01779 to 1.98599, saving model to weights-improvement-17-1.9860.hdf5\n",
      "147648/147648 [==============================] - 643s 4ms/step - loss: 1.9860\n",
      "Epoch 18/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 1.9531Epoch 00018: loss improved from 1.98599 to 1.95312, saving model to weights-improvement-18-1.9531.hdf5\n",
      "147648/147648 [==============================] - 643s 4ms/step - loss: 1.9531\n",
      "Epoch 19/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 1.9259Epoch 00019: loss improved from 1.95312 to 1.92580, saving model to weights-improvement-19-1.9258.hdf5\n",
      "147648/147648 [==============================] - 645s 4ms/step - loss: 1.9258\n",
      "Epoch 20/20\n",
      "147584/147648 [============================>.] - ETA: 0s - loss: 1.8995Epoch 00020: loss improved from 1.92580 to 1.89945, saving model to weights-improvement-20-1.8994.hdf5\n",
      "147648/147648 [==============================] - 644s 4ms/step - loss: 1.8994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd72a569550>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.8994.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" \n",
      "down, down, down. would the fall never come to an end! 'i wonder how\n",
      "many miles i've fallen by th \"\n",
      "e sooe ' she said to herself, 'ant the soont\n",
      "then i vanu to the tou th the whut toee ' \n",
      "\n",
      "'ie in tou think toe tait th the soitte'' said the caterpillar.\n",
      "\n",
      "'ie toat so the soitteen,' said the caterpillar.\n",
      "\n",
      "'i den't seinkngg the sam ' said the cotmouse, and the was a gottle\n",
      "tame to be a tald tf the tooe. '\n",
      "'\n",
      "\n",
      "\n",
      "chapter vii the soeeterg a \n",
      "iate aree aalet a\n",
      "are oo sne to tea to tee so tee to\n",
      "toeere''\n",
      "\n",
      "'ie in tou think toe ' said the cotmouse, and the was a gott eiri uiic\n",
      "seterte to tei the was hnr a mong tine tf the was so the thitg thit\n",
      "whsh and soine and thene thte the whst oo the care and the corrouse \n",
      "tas the had betnr the sabbit woile and thene thse the was all she \n",
      "sabe tar an a lote of thitg that she was all she was oo the tan oo\n",
      "tae in a lorg ofttle thieg what she was all she was oo the tas oo the\n",
      "crore of thet her an the could oo the tas and the was oo the thneg \n",
      "arolous the sas of the tan of the sabbit woide an the could so the\n",
      "tomer of the was oo the tas of the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
